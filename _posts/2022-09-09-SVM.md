---
layout : single
title : "Support Vector Machine 20220909"
---

Meshgrid, Contour


```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```


```
# meshgrid : x, y의 list를 넣어주면 grid 형태로 return #
x_m = np.arange(0,10,1)
y_m = np.arange(0,6,1)
[xmesh, ymesh] = np.meshgrid(x_m,y_m)
plt.title("Meshgrid")
plt.scatter(xmesh, ymesh)
plt.show()
```


    
![png](output_2_0.png)
    



```
# contour : level 별로 그래프 그려줌 # 
def line(x,y):
  return 2*x+y

xplt = xmesh.reshape(-1,1)
yplt = ymesh.reshape(-1,1)
zplt = line(xplt,yplt)
zmesh = zplt.reshape(xmesh.shape)
CS = plt.contour(xmesh,ymesh,zmesh,levels = 10)
plt.title("Contour")
plt.clabel(CS)
plt.show()
```


    
![png](output_3_0.png)
    


1. Linear SVM


```
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

sc = StandardScaler()
iris = load_iris()
data = iris.data
data_std = sc.fit_transform(data)

X = data[(iris['target'] == 0) | (iris['target'] == 1)][:,[2,3]]
y = iris['target'][(iris['target'] == 0) | (iris['target'] == 1)]
X_std = sc.fit_transform(X)
X_std.shape, y.shape
```




    ((100, 2), (100,))




```
plt.scatter(data_std[iris.target == 0][:,2], data_std[iris.target == 0][:,3],label = 0)
plt.scatter(data_std[iris.target == 1][:,2], data_std[iris.target == 1][:,3],label = 1)
plt.xlabel(iris.feature_names[2])
plt.ylabel(iris.feature_names[3])
plt.title("Iris Dataset")
plt.legend()
plt.show()
```


    
![png](output_6_0.png)
    



```
X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.2)
```


```
# C가 커질수록 error 가 없어지고 support vector의 개수가 줄어듦 #
import sklearn.svm as svm
svm_clf = svm.SVC(C = 1, kernel = 'linear')
svm_clf.fit(X_train, y_train)
svm_clf.predict(X_train)
```




    array([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
           1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
           0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
           1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0])




```
# +- 0.1 은 그래프에 공백을 주기 위함 #
x_range = np.arange(min(X_train[:,0])-0.1,max(X_train[:,0])+0.1,0.01)
y_range = np.arange(min(X_train[:,1])-0.1,max(X_train[:,1])+0.1,0.01)
xm, ym = np.meshgrid(x_range, y_range)

xp = xm.reshape(-1,1)
yp = ym.reshape(-1,1)
zp = svm_clf.decision_function(np.c_[xp,yp])
zm = zp.reshape(xm.shape)
```


```
# SVM Classification (Train) #

CS = plt.contour(xm, ym, zm, cmap = plt.cm.twilight)
plt.clabel(CS)
plt.scatter(X_train[svm_clf.predict(X_train)==0][:,0],X_train[svm_clf.predict(X_train)==0][:,1],label = 0, marker = '*')
plt.scatter(X_train[svm_clf.predict(X_train)==1][:,0],X_train[svm_clf.predict(X_train)==1][:,1],label = 1, marker = '*')
plt.title('Iris Classification using SVM (Train)')
plt.xlabel(iris.feature_names[2])
plt.ylabel(iris.feature_names[3])
plt.legend()
plt.show()
```


    
![png](output_10_0.png)
    



```
# Support vector 표현하고 싶다면 #

CS = plt.contour(xm, ym, zm, cmap = plt.cm.twilight,levels = [-1,0,1])
plt.clabel(CS)
plt.scatter(X_train[np.where(svm_clf.predict(X_train)==0)][:,0],X_train[np.where(svm_clf.predict(X_train)==0)][:,1],label = 0, marker = '*')
plt.scatter(X_train[np.where(svm_clf.predict(X_train)==1)][:,0],X_train[np.where(svm_clf.predict(X_train)==1)][:,1],label = 1, marker = '*')
plt.scatter(svm_clf.support_vectors_[:,0], svm_clf.support_vectors_[:,1], color = 'yellow')
plt.title('Iris Classification using SVM (Train)')
plt.xlabel(iris.feature_names[2])
plt.ylabel(iris.feature_names[3])
plt.legend()
plt.show()
```


    
![png](output_11_0.png)
    



```
# SVM Classification (Test) #
CS = plt.contour(xm, ym, zm, cmap = plt.cm.twilight)
plt.clabel(CS)
plt.scatter(X_test[svm_clf.predict(X_test)==0][:,0],X_test[svm_clf.predict(X_test)==0][:,1],label = "0 (Test)", 
            marker = '*', color = 'red', s = 200)
plt.scatter(X_test[svm_clf.predict(X_test)==1][:,0],X_test[svm_clf.predict(X_test)==1][:,1],label = "1 (Test)", 
            marker = '*', color = 'blue', s= 200)
plt.scatter(X_train[svm_clf.predict(X_train)==0][:,0],X_train[svm_clf.predict(X_train)==0][:,1], 
            alpha = 0.3, color = 'red', label = "0 (Train)")
plt.scatter(X_train[svm_clf.predict(X_train)==1][:,0],X_train[svm_clf.predict(X_train)==1][:,1], 
            alpha = 0.3, color = 'blue', label = "1 (Train")

plt.title('Iris Classification using SVM (Test)')
plt.xlabel('petal length')
plt.ylabel('petal width')
plt.legend()
plt.show()
print("SVM score is {}".format(svm_clf.score(X_test, y_test)))
```


    
![png](output_12_0.png)
    


    SVM score is 1.0
    h(x,y) = 1.2238752825123012x + 0.7610107406647881y + 0.3378636415135282 = 0
    


```
# 
y_range = np.arange(min(X_train[:,1])-0.1,max(X_train[:,1])+0.1,0.01)
x_range_plt = - (svm_clf.coef_[0,1]*y_range + svm_clf.intercept_[0]) / svm_clf.coef_[0,0]
x_range_svm1 = - (svm_clf.coef_[0,1]*y_range + svm_clf.intercept_[0] + 1) / svm_clf.coef_[0,0]
x_range_svm2 = - (svm_clf.coef_[0,1]*y_range + svm_clf.intercept_[0] - 1) / svm_clf.coef_[0,0]

plt.scatter(X_train[svm_clf.predict(X_train)==0][:,0],X_train[svm_clf.predict(X_train)==0][:,1],label = 0, marker = '*')
plt.scatter(X_train[svm_clf.predict(X_train)==1][:,0],X_train[svm_clf.predict(X_train)==1][:,1],label = 1, marker = '*')
plt.plot(x_range_plt, y_range,'r', label = "h = 0")
plt.plot(x_range_svm1, y_range,'b:', label = "h = 1", alpha = 0.5)
plt.plot(x_range_svm2, y_range,'b:', label = "h = -1", alpha = 0.5)
plt.scatter(svm_clf.support_vectors_[:,0], svm_clf.support_vectors_[:,1], color = 'yellow')
plt.legend()
plt.show()
print("h(x,y) = {}x + {}y + {}".format(svm_clf.coef_[0,0], svm_clf.coef_[0,1], svm_clf.intercept_[0]))
```


    
![png](output_13_0.png)
    


    h(x,y) = 1.2238752825123012x + 0.7610107406647881y + 0.3378636415135282
    

2. RBF SVM


```
# Circle Datset #

from sklearn import datasets
X_circle,y_circle = datasets.make_circles(n_samples = 300, shuffle = True ,
noise = 0.1, random_state = 15, factor = 0.2)
plt.figure(figsize = (8,8))
plt.title("Circle Dataset")
plt.scatter(X_circle[y_circle==0][:,0], X_circle[y_circle==0][:,1])
plt.scatter(X_circle[y_circle==1][:,0], X_circle[y_circle==1][:,1])
plt.show()
```


    
![png](output_15_0.png)
    



```
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_circle,y_circle,test_size = 0.2)

svm_circle = svm.SVC(C = 1, kernel = 'rbf', gamma = 1)
svm_circle.fit(X_train2, y_train2)
```




    SVC(C=1, gamma=1)




```
x_range2 = np.arange(min(X_train2[:,0])-0.1,max(X_train2[:,0])+0.1,0.01)
y_range2 = np.arange(min(X_train2[:,1])-0.1,max(X_train2[:,1])+0.1,0.01)
xm2, ym2 = np.meshgrid(x_range2, y_range2)

xp2 = xm2.ravel()
yp2 = ym2.ravel()
zp2 = svm_circle.decision_function(np.c_[xp2,yp2])
zm2 = zp2.reshape(xm2.shape)
```


```
plt.figure(figsize = (10,10))
CS = plt.contour(xm2, ym2, zm2, cmap = plt.cm.twilight, levels = [-1,0,1])
plt.clabel(CS)
plt.scatter(X_train2[svm_circle.predict(X_train2)==0][:,0],X_train2[svm_circle.predict(X_train2)==0][:,1],label = 0, marker = '*')
plt.scatter(X_train2[svm_circle.predict(X_train2)==1][:,0],X_train2[svm_circle.predict(X_train2)==1][:,1],label = 1, marker = '*')
plt.scatter(svm_circle.support_vectors_[:,0],svm_circle.support_vectors_[:,1],color = 'yellow')
plt.title('SVM Classification (Train, RBF, C = 1, Gamma = 1)')
plt.legend()
plt.show()
```


    
![png](output_18_0.png)
    



```
plt.figure(figsize = (10,10))
CS = plt.contour(xm2, ym2, zm2, cmap = plt.cm.twilight, levels = [-1,0,1])
plt.clabel(CS)
plt.scatter(X_test2[svm_circle.predict(X_test2)==0][:,0],X_test2[svm_circle.predict(X_test2)==0][:,1],label = "0 (Test)", marker = '*',
            s = 200, color = 'red')
plt.scatter(X_test2[svm_circle.predict(X_test2)==1][:,0],X_test2[svm_circle.predict(X_test2)==1][:,1],label = "1 (Test)", marker = '*',
            s = 200, color = 'blue')
plt.scatter(X_train2[svm_circle.predict(X_train2)==0][:,0],X_train2[svm_circle.predict(X_train2)==0][:,1],label = "0 (Train)", alpha = 0.3, color = 'red')
plt.scatter(X_train2[svm_circle.predict(X_train2)==1][:,0],X_train2[svm_circle.predict(X_train2)==1][:,1],label = "1 (Train)", alpha = 0.3, color = 'blue')
plt.title('SVM Classification (Test, RBF, C = 1, Gamma = 1)')
plt.legend()
plt.show()
print("SVM score is {}".format(svm_circle.score(X_test2, y_test2)))
```


    
![png](output_19_0.png)
    


    SVM score is 1.0
    

* Brief Visualization of How Kernel function works


```
# Kernel function : [(x*y)**(0.5), x**2, y**2] #
X_new = np.c_[2 **(0.5) * X_circle[:,0] * X_circle[:,1], X_circle[:,0]**2, X_circle[:,1]**2]
```


```
# 평면 h를 구하기 #
y_new = np.zeros(len(X_new))
for i in range(len(X_new)):
  if X_new[i,1] + X_new[i,2] < 0.5:
    y_new[i] = 0
  else: 
    y_new[i] = 1

svm_test = svm.SVC(C = 1, kernel = 'linear')
svm_test.fit(X_new, y_new)

x_poly = np.linspace(min(X_new[:,0]), max(X_new[:,0]), 30)
y_poly = np.linspace(min(X_new[:,1]), max(X_new[:,1]), 30)
xm_poly, ym_poly = np.meshgrid(x_poly, y_poly)
xm_plt = xm_poly.reshape(-1,1)
ym_plt = ym_poly.reshape(-1,1)
zm_plt = (svm_test.coef_[0,0]*xm_plt + svm_test.coef_[0,1]*ym_plt + svm_test.intercept_) / (- svm_test.coef_[0,2])
zm_poly = zm_plt.reshape(xm_poly.shape)
```


```
figure_k = plt.figure(figsize = (15,5))

ax_1 = plt.subplot(1,2,1, projection="3d")
ax_1.set_title("How Kernel works")
ax_1.scatter(X_new[X_new[:,1] + X_new[:,2] < 0.5][:,0], X_new[X_new[:,1] + X_new[:,2] < 0.5][:,1], X_new[X_new[:,1] + X_new[:,2] < 0.5][:,2], label = 0)
ax_1.scatter(X_new[X_new[:,1] + X_new[:,2] >= 0.5][:,0], X_new[X_new[:,1] + X_new[:,2] >= 0.5][:,1], X_new[X_new[:,1] + X_new[:,2] >= 0.5][:,2], label = 1)
ax_1.plot_surface(xm_poly, ym_poly, zm_poly, alpha = 0.5) 
ax_1.set_xlabel("X * y * 2 **(0.5)")
ax_1.set_ylabel("X ** 2")
ax_1.set_zlabel("y ** 2")

ax_2 = plt.subplot(1,2,2, projection="3d")
ax_2.set_title("From different angle")
ax_2.scatter(X_new[X_new[:,1] + X_new[:,2] < 0.5][:,0], X_new[X_new[:,1] + X_new[:,2] < 0.5][:,1], X_new[X_new[:,1] + X_new[:,2] < 0.5][:,2], label = 0)
ax_2.scatter(X_new[X_new[:,1] + X_new[:,2] >= 0.5][:,0], X_new[X_new[:,1] + X_new[:,2] >= 0.5][:,1], X_new[X_new[:,1] + X_new[:,2] >= 0.5][:,2], label = 1)
ax_2.plot_surface(xm_poly, ym_poly, zm_poly, alpha = 0.5)
ax_2.view_init(0,0) 
ax_2.xaxis.set_ticklabels([])
ax_2.set_ylabel("X ** 2")
ax_2.set_zlabel("y ** 2")

plt.show()
```


    
![png](output_23_0.png)
    



```
# C 가 높아질수록 test, train의 score가 높아진다 #
score_list_C = []
for i in range(10):
  svm_test_C = svm.SVC(C = 10**(i-5), kernel = 'rbf', gamma = 0.1)
  svm_test_C.fit(X_train2, y_train2)
  score_list_C.append([svm_test_C.score(X_test2, y_test2), svm_test_C.score(X_train2, y_train2)])
plt.plot(np.arange(len(score_list_C)),np.array(score_list_C)[:,0], label = "Test Score")
plt.plot(np.arange(len(score_list_C)),np.array(score_list_C)[:,1], label = "Train Score")
plt.title("SVM Score by C (RBF, gamma = 0.1)")
plt.xticks(np.arange(len(score_list_C)),[(i-5) for i in range(10)])
plt.xlabel("log C")
plt.ylabel("Score")
plt.show()
```


    
![png](output_24_0.png)
    



```
# Gamma 가 높아질수록 train 데이터 하나하나에 민감해져 과적합에 빠진다 #
score_list_G = []
for i in range(10):
  svm_test_G = svm.SVC(C = 1, kernel = 'rbf', gamma = 10**(i-5))
  svm_test_G.fit(X_train2, y_train2)
  score_list_G.append([svm_test_G.score(X_test2, y_test2), svm_test_G.score(X_train2, y_train2)])
plt.plot(np.arange(len(score_list_G)),np.array(score_list_G)[:,0], label = "Test Score")
plt.plot(np.arange(len(score_list_G)),np.array(score_list_G)[:,1], label = "Train Score")
plt.title("SVM Score by Gamma (RBF, C = 1)")
plt.xticks(np.arange(len(score_list_G)),[(i-5) for i in range(10)])
plt.xlabel("log G")
plt.ylabel("Score")
plt.show()
```


    
![png](output_25_0.png)
    


3. Poly SVM


```
# Moon Dataset #

from sklearn import datasets
X_moon,y_moon = datasets.make_moons(n_samples = 300, shuffle = True ,
noise = 0.1, random_state = 15)
plt.figure(figsize = (10,8))
plt.title("Moon Dataset")
plt.scatter(X_moon[y_moon==0][:,0], X_moon[y_moon==0][:,1])
plt.scatter(X_moon[y_moon==1][:,0], X_moon[y_moon==1][:,1])
plt.show()
```


    
![png](output_27_0.png)
    



```
X_train3, X_test3, y_train3, y_test3 = train_test_split(X_moon,y_moon,test_size = 0.2)

svm_moon = svm.SVC(kernel = 'poly', coef0 = 1, C = 5, degree = 3, gamma = 1)
svm_moon.fit(X_train3, y_train3)
```




    SVC(C=5, coef0=1, gamma=1, kernel='poly')




```
x_range3 = np.arange(min(X_train3[:,0])-0.1, max(X_train3[:,0])+0.1,0.01)
y_range3 = np.arange(min(X_train3[:,1])-0.1, max(X_train3[:,1])+0.1,0.01)
xm3,ym3 = np.meshgrid(x_range3, y_range3)
xp3 = xm3.reshape(-1,1)
yp3 = ym3.reshape(-1,1)
zp3 = svm_moon.decision_function(np.c_[xp3, yp3])
zm3 = zp3.reshape(xm3.shape)
```


```
plt.figure(figsize = (8,8))
CS = plt.contour(xm3,ym3,zm3, cmap = plt.cm.twilight, levels = [-1,0,1])
plt.clabel(CS)
plt.scatter(X_train3[svm_moon.predict(X_train3) == 0][:,0],X_train3[svm_moon.predict(X_train3) == 0][:,1], label = 0)
plt.scatter(X_train3[svm_moon.predict(X_train3) == 1][:,0],X_train3[svm_moon.predict(X_train3) == 1][:,1], label = 1)
plt.scatter(svm_moon.support_vectors_[:,0],svm_moon.support_vectors_[:,1], color = 'yellow')
plt.title('SVM Classification (Train, Poly, C = 5, Gamma = 1)')
plt.legend()
plt.show()
```


    
![png](output_30_0.png)
    



```
plt.figure(figsize = (8,8))
CS = plt.contour(xm3, ym3, zm3, cmap = plt.cm.twilight)
plt.clabel(CS)
plt.scatter(X_train3[svm_moon.predict(X_train3) == 0][:,0],X_train3[svm_moon.predict(X_train3) == 0][:,1],
            alpha = 0.3, color = 'red', label = "0 (Train)")
plt.scatter(X_train3[svm_moon.predict(X_train3) == 1][:,0],X_train3[svm_moon.predict(X_train3) == 1][:,1],
            alpha = 0.3, color = 'blue', label = "1 (Train")
plt.scatter(X_test3[svm_moon.predict(X_test3) == 0][:,0],X_test3[svm_moon.predict(X_test3) == 0][:,1], marker = '*',s = 200,
            label = "0 (Test)", color = 'red')
plt.scatter(X_test3[svm_moon.predict(X_test3) == 1][:,0],X_test3[svm_moon.predict(X_test3) == 1][:,1], marker = '*',s = 200,
            label = "1 (Test)", color = 'blue')
plt.title('SVM Classification (Test, Poly, C = 5, Gamma = 1)')
plt.legend()
plt.show()
print("SVM score is {}".format(svm_moon.score(X_test3, y_test3)))
```


    
![png](output_31_0.png)
    


    SVM score is 1.0
    
